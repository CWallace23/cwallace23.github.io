---
layout: post
title: "Students' example use"
---

I'm interested in how students think about examples, and whether we can train them to "think better" about examples. At the moment this is all just hypothetical, and I'm hoping to formalise it in the future. 

There's a lot of research out there about example generation tasks. The [book by Watson and Mason](https://www.taylorfrancis.com/books/mono/10.4324/9781410613714/mathematics-constructive-activity-anne-watson-john-mason) is the obvious starting point, and there are also loads of good papers.

I think I'm more interested in the idea of getting students to use examples to help them to understand a difficult new piece of mathematics. This is difficult to observe, and practically impossible to write a paper on, so is most likely just a good piece of advice to give them rather than anything I can publish.

Suppose you're looking through some lecture notes, and you come across:

> **Theorem 2.15**. Let $$(X_k)_{k\geq 1}$$ be i.i.d. random variables with values in $$\{0, 1, 2, . . . }$$ and let $$n \geq 0$$ be an integer-valued random variable independent of $$\{X_k\}_{k\geq 1}$$. Then $$S_N := X_1 + \dots + X_N$$ has generating function
$$ G_{S_N}(s) = G_N \big( G_X(s)\big). $$

How should you unravel this? My advice is to come up with three examples, following the process "the good, the bad, and the ugly".

#### The good

First, we replace each mathematical object with the nicest example we can think of. This will help us to see roughly what's going on, as long as everything is nice.

For instance, the first Hilbert space I think of is $$\mathbb{R}^2$$ (you could use $$\mathbb{R}$$ here, but in one dimension there's not enough space for anything to be orthogonal to anything else).

A very basic $$A \subset \mathbb{R}^2$$, is the real line $$ \mathbb{R} = \{ (\alpha,0) \}$$. Then its orthogonal complement is everything perpendicular to the $$x$$-axis,  $$A^{\perp} = \{ (0, \beta)\}$$ which I *am* willing to believe is a closed subspace of $$\mathbb{R}^2$$.

#### The bad

Now we stretch the theorem by using a "weird" example for each mathematical object. If we're not feeling confident we can test out one mathematical object at a time. This helps us to start to see how the conditions of the theorem apply, in increasingly detailed ways.

It's a while since I've thought about Hilbert spaces, but I know you can use spaces of functions. Let's take $$\mathcal{H}$$ as the space of complex-valued, measurable functions on $$[0,1]$$ (if we were really committing to "weird" we could use $$\mathbb{C}^n$$), with the inner product defined as
\[ <f,g> = \int_0^1 f(x) \bar{g(x)} dx.\]

For a subset $$A \subset \mathcal{H}$$ we could use the set of functions that are only supported on $$[0, \frac{1}{2}]$$ (so their value is equal to 0 on $$[\frac{1}{2},0]$$). The set of functions orthogonal to everything in $$A$$ is everything that is only supported on $$[\frac{1}{2}, 0]$$ - again, we've found a closed subset of $$\mathcal{H}$$. 

#### The ugly

Finally, we see what happens when the conditions of the theorem aren't fulfilled. What happens if we use an example that *isn't* a Hilbert space? We could use the upper half-plane. This isn't a Hilbert space, because it isn't a vector space at all (it's not closed under multiplication by -1). 

When we take our example from before, and substitute in the $$x$$-axis for $$A$$, we find that $$A^{\perp}$$ is just the upper bit of the $$y$$-axis, and it's not a closed subspace (because we can multiply elements by -1, and get something that is not in $$A^{\perp}$$). 

